{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
    "                             confusion_matrix, roc_curve, auc, precision_recall_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "df_test = pd.read_csv(\"data/fina/test_data.csv\")\n",
    "\n",
    "# Drop classification target to avoid leakage\n",
    "df_test = df_test.drop(columns=['DepDelayMinutes'])\n",
    "\n",
    "# Extract features and target\n",
    "X_test = df_test.drop(columns=['DepDelay'])\n",
    "y_test = df_test['DepDelay'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression models\n",
    "classification_models = {\n",
    "    \"Linear Regression\": \"models/logistic_regression/logistic_regression.pkl\",\n",
    "    \"SGD Regression\": \"models/sgd_classifier/sgd_classifier.pkl\",\n",
    "    \"HistGradient Regression\": \"models/histgradient_classifier/histgradient_classifier.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "classification_results = []\n",
    "model_predictions = {}\n",
    "\n",
    "# Load models and evaluate\n",
    "for model_name, file in classification_models.items():\n",
    "    with open(file, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    model_predictions[model_name] = y_pred\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):  # Probabilistic classifiers\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]  \n",
    "        model_predictions[model_name + \" Prob\"] = y_proba\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    classification_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df_classification = pd.DataFrame(classification_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization ---\n",
    "\n",
    "# 1. Classification Performance Bar Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_classification.set_index(\"Model\").plot(kind=\"bar\", figsize=(10, 6))\n",
    "plt.title(\"Classification Model Performance Comparison\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion Matrices\n",
    "for model_name in classification_models.keys():\n",
    "    cm = confusion_matrix(y_test, model_predictions[model_name])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# 3. ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model_name in classification_models.keys():\n",
    "    if model_name + \" Prob\" in model_predictions:\n",
    "        fpr, tpr, _ = roc_curve(y_test, model_predictions[model_name + \" Prob\"])\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc(fpr, tpr):.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model_name in classification_models.keys():\n",
    "    if model_name + \" Prob\" in model_predictions:\n",
    "        precision, recall, _ = precision_recall_curve(y_test, model_predictions[model_name + \" Prob\"])\n",
    "        plt.plot(recall, precision, label=model_name)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
